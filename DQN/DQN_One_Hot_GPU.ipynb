{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity('ERROR')\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from collections import defaultdict, deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from Environment_one_hot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location and directions of cells are: \n",
      "{1: [0, 0, 0], 2: [0, 0, 1], 3: [0, 0, 2], 4: [1500.0, 0, 0], 5: [1500.0, 0, 1], 6: [1500.0, 0, 2], 7: [750.0, 1250.0, 0], 8: [750.0, 1250.0, 1], 9: [750.0, 1250.0, 2], 10: [-750.0, 1250.0, 0], 11: [-750.0, 1250.0, 1], 12: [-750.0, 1250.0, 2], 13: [-1500.0, 0, 0], 14: [-1500.0, 0, 1], 15: [-1500.0, 0, 2], 16: [-750.0, -1250.0, 0], 17: [-750.0, -1250.0, 1], 18: [-750.0, -1250.0, 2], 19: [750.0, -1250.0, 0], 20: [750.0, -1250.0, 1], 21: [750.0, -1250.0, 2]} \n",
      " \n",
      "\n",
      "Strongest cells for sector (0, 0)\n",
      "[[13.0, -66.13575189324237], [5.0, -69.39825619992635], [20.0, -78.47993769564032], [16.0, -79.0159991699583], [10.0, -79.39007903732177], [9.0, -81.35188256835082]]\n",
      "\n",
      "\n",
      "Strongest cells for sector (0, 100)\n",
      "[[5.0, -68.47266999370427], [13.0, -72.12584635478302], [2.0, -77.82938129185086], [17.0, -82.16166605867052], [20.0, -84.53061862956298], [9.0, -86.10679360619605]]\n",
      "\n",
      "\n",
      "Strongest cells for sector (50, 0)\n",
      "[[5.0, -69.94999810184717], [13.0, -72.42896910919771], [16.0, -80.08780154442617], [10.0, -81.16877667551843], [9.0, -82.8568900558623], [20.0, -84.71564625201576]]\n",
      "\n",
      "\n",
      "Strongest cells for sector (50, 100)\n",
      "[[5.0, -72.72590809188888], [13.0, -73.71393414084986], [16.0, -82.60267188338324], [10.0, -84.2548381350624], [20.0, -84.48897189201807], [2.0, -93.59488182761788]]\n",
      "\n",
      "\n",
      "[-79.14246398 -79.65123424 -86.79651215 ... -85.8765658  -93.41295539\n",
      " -97.11853592]\n",
      "Maximum RSRP value = -38.53184989319559, Minimum RSRP value = -166.4229491084165\n",
      "New maximum RSRP value = 1.0, New minimum RSRP value = 0.0\n"
     ]
    }
   ],
   "source": [
    "LENGTH = 6000\n",
    "WIDTH = 5000\n",
    "DIVISION = 50\n",
    "K = 6\n",
    "NUM_BASE_STATIONS = 7\n",
    "\n",
    "env = Environment(LENGTH, WIDTH, DIVISION, 1, 0, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, k, NUM_BASE_STATIONS):\n",
    "        self.k = k\n",
    "        cells = 3*NUM_BASE_STATIONS\n",
    "        num_inputs = 3 + cells\n",
    "        # Model\n",
    "        #--------------------------------------------------------------------\n",
    "        input_A = Input(shape = num_inputs) #(x, y, direction, current_serving_cell_one_hot_encoded)\n",
    "        x = Dense(64, activation = 'relu')(input_A)\n",
    "#         x = Dense(64, activation = 'relu')(x)\n",
    "        x = Dense(32, activation = 'relu')(x)\n",
    "        x = Dense(self.k)(x)\n",
    "        \n",
    "        self.model = Model(inputs = input_A, outputs = x)\n",
    "        print(self.model.summary())\n",
    "        #--------------------------------------------------------------------\n",
    "        \n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.loss_fn = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr = 0.05)\n",
    "        \n",
    "        self.batch_size = 64\n",
    "        self.epsilon = 0.3\n",
    "        self.gamma = 0.3\n",
    "        \n",
    "        #Replay Buffers\n",
    "        self.action_history = deque(maxlen = 10000)\n",
    "        self.state_history = deque(maxlen = 10000)\n",
    "        self.next_state_history = deque(maxlen = 10000)\n",
    "        self.rewards_history = deque(maxlen = 10000)\n",
    "        self.done_history = deque(maxlen = 10000)\n",
    "\n",
    "        \n",
    "    def play_one_step(self, state, route, dest, Wrsrp, Who):\n",
    "        action = self.exp_policy(state)\n",
    "        next_state, reward, done, change = env.step(state, route, action, dest)\n",
    "        next_state = list(next_state)\n",
    "        \n",
    "        reward*=Wrsrp\n",
    "        reward-=change*Who\n",
    "        \n",
    "        self.append_replay_buffer(state, action, next_state, reward, done)\n",
    "        return next_state, reward, done, change\n",
    "    \n",
    "    def exp_policy(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(self.k)\n",
    "        else:\n",
    "            normalised_state = self.normalise_inputs(np.array(state)[np.newaxis])\n",
    "            Q_values = self.model(normalised_state)\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    def append_replay_buffer(self, state, action, next_state, reward, done):\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        self.next_state_history.append(next_state)\n",
    "        self.rewards_history.append(reward)\n",
    "        self.done_history.append(done)\n",
    "        \n",
    "    def sample_experience(self):\n",
    "        indices = np.random.randint(len(self.state_history), size = self.batch_size)\n",
    "        \n",
    "        states = np.array([self.state_history[i] for i in indices])\n",
    "        actions = np.array([self.action_history[i] for i in indices])\n",
    "        next_states = np.array([self.next_state_history[i] for i in indices])\n",
    "        rewards = np.array([self.rewards_history[i] for i in indices])\n",
    "        dones = np.array([self.done_history[i] for i in indices])\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "        \n",
    "    \n",
    "    def training_step(self, num_training_episode):\n",
    "        for _ in range(num_training_episode):\n",
    "            states, actions, next_states, rewards, dones = self.sample_experience()\n",
    "\n",
    "            states = self.normalise_inputs(states)\n",
    "            next_states = self.normalise_inputs(next_states)\n",
    "\n",
    "            next_Q_values = self.target_model(next_states)\n",
    "            max_next_Q_values = np.max(next_Q_values, axis= 1)\n",
    "\n",
    "            target_Q_values = rewards + (1-dones)*self.gamma*max_next_Q_values\n",
    "            mask = tf.one_hot(actions, self.k)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                all_Q_values = self.model(states)\n",
    "                Q_values = tf.reduce_sum(all_Q_values*mask, axis = 1, keepdims = True)\n",
    "                loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        \n",
    "    def normalise_inputs(self, states):\n",
    "        '''Normalising the inputs to the NN'''\n",
    "        states = states.astype('float')\n",
    "        states[:,0]/=(LENGTH/2)\n",
    "        states[:,1]/=(WIDTH/2)\n",
    "        states[:,2]/=8\n",
    "        states = tf.convert_to_tensor(states)\n",
    "#         print(states)\n",
    "        \n",
    "        return states\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1600      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 3,878\n",
      "Trainable params: 3,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(6, NUM_BASE_STATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                           | 295/10000 [00:28<15:50, 10.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bf925786a97d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWrsrp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWho\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mtotal_reward\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-952e7edec098>\u001b[0m in \u001b[0;36mplay_one_step\u001b[1;34m(self, state, route, dest, Wrsrp, Who)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplay_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWrsrp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWho\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-952e7edec098>\u001b[0m in \u001b[0;36mexp_policy\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mnormalised_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalise_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mQ_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalised_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mappend_replay_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \"\"\"\n\u001b[1;32m-> 1186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Wrsrp = 1\n",
    "Who = 1\n",
    "last_r = []\n",
    "rewards = []\n",
    "r = 100\n",
    "num_training_episode = 1\n",
    "\n",
    "for episode in tqdm(range(10000)):\n",
    "    src,dest = env.give_src_dest()\n",
    "    route = env.compute_route(src, dest)\n",
    "    state = route.popleft()\n",
    "    depth = 3*NUM_BASE_STATIONS\n",
    "    one_hot_cell = make_one_hot(env.sector_cells[src][0][0], depth)\n",
    "    state.extend(one_hot_cell) #Setting strongest cell as the initial serving cell (one_hot)\n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    action = 0\n",
    "#     print(state)\n",
    "    \n",
    "    while done==0:\n",
    "        next_state, reward, done, change = agent.play_one_step(state, route, dest, Wrsrp, Who)\n",
    "        total_reward+=reward\n",
    "        state = next_state\n",
    "    \n",
    "    last_r.append(total_reward)\n",
    "    \n",
    "    if not episode%r:\n",
    "        rewards.append(np.average(np.array(last_r)))\n",
    "        last_r = []\n",
    "        \n",
    "    if episode>50:\n",
    "        agent.training_step(num_training_episode)\n",
    "    \n",
    "    if episode%20==0:\n",
    "        agent.target_model.set_weights(agent.model.get_weights())\n",
    "    \n",
    "    if episode%100==0:\n",
    "        agent.epsilon*=0.9\n",
    "        if agent.epsilon<0.01:\n",
    "            agent.epsilon = 0.01\n",
    "            \n",
    "    if episode%1000==0:\n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "        \n",
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "print(rewards[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hos = []\n",
    "agent.epsilon = 0\n",
    "for episode in tqdm(range(2000)):\n",
    "    src,dest = env.give_src_dest()\n",
    "    route = env.compute_route(src, dest)\n",
    "    state = route.popleft()\n",
    "    state.append(env.sector_cells[src][0][0]) #Setting strongest cell as the initial serving cell\n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    num_hos = 0\n",
    "    \n",
    "    while done==0:\n",
    "        next_state, reward, done, change = agent.play_one_step(state, route, dest, Wrsrp, Who)\n",
    "        total_reward+=reward\n",
    "        state = next_state\n",
    "        if change:\n",
    "            num_hos +=1\n",
    "    hos.append(num_hos)\n",
    "    \n",
    "print(np.mean(hos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
