{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity('ERROR')\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from collections import defaultdict, deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from Environment_one_hot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LENGTH = 6000\n",
    "WIDTH = 5000\n",
    "DIVISION = 50\n",
    "K = 6\n",
    "NUM_BASE_STATIONS = 7\n",
    "\n",
    "env = Environment(LENGTH, WIDTH, DIVISION, 1, 0, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, k, NUM_BASE_STATIONS):\n",
    "        self.k = k\n",
    "        cells = 3*NUM_BASE_STATIONS\n",
    "        num_inputs = 2 + 8 + cells\n",
    "        # Model\n",
    "        #--------------------------------------------------------------------\n",
    "        input_A = Input(shape = num_inputs) #(x, y, direction, current_serving_cell_one_hot_encoded)\n",
    "        x = Dense(32)(input_A)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dense(64)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dense(32)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dense(self.k)(x)\n",
    "        \n",
    "        self.model = Model(inputs = input_A, outputs = x)\n",
    "        print(self.model.summary())\n",
    "        #--------------------------------------------------------------------\n",
    "        \n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.loss_fn = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr = 0.0005)\n",
    "        \n",
    "        self.batch_size = 1024\n",
    "        self.epsilon = 1\n",
    "        self.gamma = 0.3\n",
    "        self.replay_buffer_size = 10240\n",
    "        \n",
    "        #Replay Buffers\n",
    "        self.action_history = deque(maxlen = self.replay_buffer_size)\n",
    "        self.state_history = deque(maxlen = self.replay_buffer_size)\n",
    "        self.next_state_history = deque(maxlen = self.replay_buffer_size)\n",
    "        self.rewards_history = deque(maxlen = self.replay_buffer_size)\n",
    "        self.done_history = deque(maxlen = self.replay_buffer_size)\n",
    "\n",
    "        \n",
    "    def play_one_step(self, state, route, dest, Wrsrp, Who, baseline = False):\n",
    "        if not baseline:\n",
    "            action = self.exp_policy(state)\n",
    "        else:\n",
    "            action = 0\n",
    "            \n",
    "        next_state, reward, done, change = env.step(state, route, action, dest)\n",
    "        next_state = list(next_state)\n",
    "        reward*=Wrsrp\n",
    "        reward-=change*Who\n",
    "        \n",
    "        self.append_replay_buffer(state, action, next_state, reward, done)\n",
    "        return next_state, reward, done, change\n",
    "    \n",
    "    def exp_policy(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(self.k)\n",
    "        else:\n",
    "            normalised_state = self.normalise_inputs(np.array(state)[np.newaxis])\n",
    "            Q_values = self.model(normalised_state)\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    def append_replay_buffer(self, state, action, next_state, reward, done):\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        self.next_state_history.append(next_state)\n",
    "        self.rewards_history.append(reward)\n",
    "        self.done_history.append(done)\n",
    "        \n",
    "    def sample_experience(self):\n",
    "        indices = np.random.randint(len(self.state_history), size = self.batch_size)\n",
    "        \n",
    "        states = np.array([self.state_history[i] for i in indices])\n",
    "        actions = np.array([self.action_history[i] for i in indices])\n",
    "        next_states = np.array([self.next_state_history[i] for i in indices])\n",
    "        rewards = np.array([self.rewards_history[i] for i in indices])\n",
    "        dones = np.array([self.done_history[i] for i in indices])\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "        \n",
    "    \n",
    "    def training_step(self, num_training_episode):\n",
    "        for _ in range(num_training_episode):\n",
    "            states, actions, next_states, rewards, dones = self.sample_experience()\n",
    "\n",
    "            states = self.normalise_inputs(states)\n",
    "            next_states = self.normalise_inputs(next_states)\n",
    "            selected_actions = np.argmax(self.model(next_states), axis = 1)\n",
    "            \n",
    "            \n",
    "            max_next_Q_values = np.array(self.target_model(next_states))[:, selected_actions]\n",
    "            \n",
    "#             max_next_Q_values = np.max(next_Q_values, axis= 1)\n",
    "\n",
    "            target_Q_values = rewards + (1-dones)*self.gamma*max_next_Q_values\n",
    "            mask = tf.one_hot(actions, self.k)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                all_Q_values = self.model(states)\n",
    "                Q_values = tf.reduce_sum(all_Q_values*mask, axis = 1, keepdims = True)\n",
    "                loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        \n",
    "    def normalise_inputs(self, states):\n",
    "        '''Normalising the inputs to the NN'''\n",
    "        states = states.astype('float')\n",
    "        states[:,0]/=(LENGTH/2)\n",
    "        states[:,1]/=(WIDTH/2)\n",
    "        states = tf.convert_to_tensor(states)\n",
    "        \n",
    "        return states\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent_0 = Agent(6, NUM_BASE_STATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = agent_0\n",
    "Wrsrp = 1\n",
    "Who = 0\n",
    "last_r = []\n",
    "rewards = []\n",
    "hos = []\n",
    "last_r_hos = []\n",
    "r = 100\n",
    "num_training_episode = 1 #training steps per episode\n",
    "target_model_update = 100\n",
    "max_reward = float('-inf')\n",
    "path = 'agent_0.h5'\n",
    "\n",
    "for episode in tqdm(range(20000)):\n",
    "    src,dest = env.give_src_dest()\n",
    "    route = env.compute_route(src, dest)\n",
    "    state = route.popleft()\n",
    "    depth = 3*NUM_BASE_STATIONS\n",
    "    one_hot_cell = make_one_hot(env.sector_cells[src][0][0], depth)\n",
    "    one_hot_direction = make_one_hot(state[-1]+1, 8)\n",
    "    state = state[:-1]\n",
    "    state.extend(one_hot_direction)\n",
    "    state.extend(one_hot_cell) #Setting strongest cell as the initial serving cell (one_hot)\n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    action = 0\n",
    "    num_hos = 0\n",
    "    \n",
    "    while done==0:\n",
    "        next_state, reward, done, change = agent.play_one_step(state, route, dest, Wrsrp, Who)\n",
    "        total_reward+=reward\n",
    "        if change:\n",
    "            num_hos+=1\n",
    "        state = next_state\n",
    "        \n",
    "    \n",
    "    last_r_hos.append(num_hos)\n",
    "    last_r.append(total_reward)\n",
    "    \n",
    "    if not episode%r:\n",
    "        rewards.append(np.average(np.array(last_r)))\n",
    "        hos.append(np.mean(last_r_hos))\n",
    "        last_r = []\n",
    "        last_r_hos = []\n",
    "        \n",
    "    if episode>50:\n",
    "        agent.training_step(num_training_episode)\n",
    "        \n",
    "        if rewards[-1]>max_reward and episode>1000:\n",
    "            max_reward = rewards[-1]\n",
    "            agent.model.save_weights(path)\n",
    "            print(f'Saved new weights for reward of {max_reward}')\n",
    "            \n",
    "    \n",
    "    if episode%target_model_update==0:\n",
    "        agent.target_model.set_weights(agent.model.get_weights())\n",
    "    \n",
    "    if episode%200==0:\n",
    "        agent.epsilon*=0.9\n",
    "        agent.epsilon = min(agent.epsilon, 0.05)\n",
    "            \n",
    "    if episode%1000==0:\n",
    "        plt.plot(rewards)\n",
    "        plt.title('Average Rewards')\n",
    "        plt.show()\n",
    "        plt.plot(hos)\n",
    "        plt.title('Average Handovers')\n",
    "        plt.show()\n",
    "        \n",
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "print(rewards[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.load_weights(path)\n",
    "hos = []\n",
    "rewards = []\n",
    "agent.epsilon = 0\n",
    "for episode in tqdm(range(2000)):\n",
    "    src,dest = env.give_src_dest()\n",
    "    route = env.compute_route(src, dest)\n",
    "    state = route.popleft()\n",
    "    depth = 3*NUM_BASE_STATIONS\n",
    "    one_hot_cell = make_one_hot(env.sector_cells[src][0][0], depth)\n",
    "    one_hot_direction = make_one_hot(state[-1]+1, 8)\n",
    "    state = state[:-1]\n",
    "    state.extend(one_hot_direction)\n",
    "    state.extend(one_hot_cell) #Setting strongest cell as the initial serving cell (one_hot)\n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    num_hos = 0\n",
    "    \n",
    "    while done==0:\n",
    "        next_state, reward, done, change = agent.play_one_step(state, route, dest, Wrsrp, Who)\n",
    "        total_reward+=reward\n",
    "        if change:\n",
    "            num_hos +=1\n",
    "        state = next_state\n",
    "        \n",
    "    rewards.append(total_reward)\n",
    "    hos.append(num_hos)\n",
    "\n",
    "agent.mean_reward = np.mean(rewards) \n",
    "agent.mean_hos = np.mean(hos)\n",
    "\n",
    "print(agent.mean_reward)\n",
    "print(agent.mean_hos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent_19 = Agent(6, NUM_BASE_STATIONS)\n",
    "#lr = 0.005, 0.001, batch_size = 256, same for agent_0\n",
    "#epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = agent_19\n",
    "Wrsrp = 1\n",
    "Who = 1/9\n",
    "last_r = []\n",
    "rewards = []\n",
    "hos = []\n",
    "last_r_hos = []\n",
    "r = 100\n",
    "num_training_episode = 1 #training steps per episode\n",
    "target_model_update = 100\n",
    "max_reward = float('-inf')\n",
    "path = 'agent_19.h5'\n",
    "\n",
    "for episode in tqdm(range(20000)):\n",
    "    src,dest = env.give_src_dest()\n",
    "    route = env.compute_route(src, dest)\n",
    "    state = route.popleft()\n",
    "    depth = 3*NUM_BASE_STATIONS\n",
    "    one_hot_cell = make_one_hot(env.sector_cells[src][0][0], depth)\n",
    "    one_hot_direction = make_one_hot(state[-1]+1, 8)\n",
    "    state = state[:-1]\n",
    "    state.extend(one_hot_direction)\n",
    "    state.extend(one_hot_cell) #Setting strongest cell as the initial serving cell (one_hot)\n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    action = 0\n",
    "    num_hos = 0\n",
    "\n",
    "    \n",
    "    while done==0:\n",
    "        next_state, reward, done, change = agent.play_one_step(state, route, dest, Wrsrp, Who)\n",
    "        total_reward+=reward\n",
    "        state = next_state\n",
    "        if change:\n",
    "            num_hos+=1\n",
    "    \n",
    "    last_r_hos.append(num_hos)\n",
    "    last_r.append(total_reward)\n",
    "    \n",
    "    if not episode%r:\n",
    "        rewards.append(np.average(np.array(last_r)))\n",
    "        hos.append(np.mean(last_r_hos))\n",
    "        last_r = []\n",
    "        last_r_hos = []\n",
    "        \n",
    "    if episode>50:\n",
    "        agent.training_step(num_training_episode)\n",
    "        \n",
    "        if rewards[-1]>max_reward and episode>1000:\n",
    "            max_reward = rewards[-1]\n",
    "            agent.model.save_weights(path)\n",
    "            print(f'Saved new weights for reward of {max_reward}')\n",
    "            \n",
    "    \n",
    "    if episode%target_model_update==0:\n",
    "        agent.target_model.set_weights(agent.model.get_weights())\n",
    "    \n",
    "    if episode%200==0:\n",
    "        agent.epsilon*=0.9\n",
    "        agent.epsilon = min(agent.epsilon, 0.05)\n",
    "            \n",
    "    if episode%1000==0:\n",
    "        plt.plot(rewards)\n",
    "        plt.title('Average Rewards')\n",
    "        plt.show()\n",
    "        plt.plot(hos)\n",
    "        plt.title('Average Handovers')\n",
    "        plt.show()\n",
    "        \n",
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "print(rewards[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.load_weights(path)\n",
    "hos = []\n",
    "rewards = []\n",
    "agent.epsilon = 0\n",
    "for episode in tqdm(range(2000)):\n",
    "    src,dest = env.give_src_dest()\n",
    "    route = env.compute_route(src, dest)\n",
    "    state = route.popleft()\n",
    "    depth = 3*NUM_BASE_STATIONS\n",
    "    one_hot_cell = make_one_hot(env.sector_cells[src][0][0], depth)\n",
    "    one_hot_direction = make_one_hot(state[-1]+1, 8)\n",
    "    state = state[:-1]\n",
    "    state.extend(one_hot_direction)\n",
    "    state.extend(one_hot_cell) #Setting strongest cell as the initial serving cell (one_hot)\n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    num_hos = 0\n",
    "    \n",
    "    while done==0:\n",
    "        next_state, reward, done, change = agent.play_one_step(state, route, dest, Wrsrp, Who)\n",
    "        total_reward+=reward\n",
    "        state = next_state\n",
    "        if change:\n",
    "            num_hos +=1\n",
    "    rewards.append(total_reward)\n",
    "    hos.append(num_hos)\n",
    "\n",
    "agent.mean_reward = np.mean(rewards) \n",
    "agent.mean_hos = np.mean(hos)\n",
    "\n",
    "print(agent.mean_reward)\n",
    "print(agent.mean_hos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1_1 = Agent(6, NUM_BASE_STATIONS)\n",
    "#lr = 0.0005, batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = agent_1_1\n",
    "Wrsrp = 1\n",
    "Who = 1\n",
    "last_r = []\n",
    "rewards = []\n",
    "hos = []\n",
    "last_r_hos = []\n",
    "r = 100\n",
    "num_training_episode = 1\n",
    "target_model_update = 100\n",
    "max_reward = float('-inf')\n",
    "path = 'agent_1_1.h5'\n",
    "\n",
    "for episode in tqdm(range(20000)):\n",
    "    src,dest = env.give_src_dest()\n",
    "    route = env.compute_route(src, dest)\n",
    "    state = route.popleft()\n",
    "    depth = 3*NUM_BASE_STATIONS\n",
    "    one_hot_cell = make_one_hot(env.sector_cells[src][0][0], depth)\n",
    "    one_hot_direction = make_one_hot(state[-1]+1, 8)\n",
    "    state = state[:-1]\n",
    "    state.extend(one_hot_direction)\n",
    "    state.extend(one_hot_cell) #Setting strongest cell as the initial serving cell (one_hot)\n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    action = 0\n",
    "    num_hos = 0\n",
    "    \n",
    "    while done==0:\n",
    "        next_state, reward, done, change = agent.play_one_step(state, route, dest, Wrsrp, Who)\n",
    "        total_reward+=reward\n",
    "        state = next_state\n",
    "        if change:\n",
    "            num_hos+=1\n",
    "    \n",
    "    last_r_hos.append(num_hos)\n",
    "    last_r.append(total_reward)\n",
    "    \n",
    "    if not episode%r:\n",
    "        rewards.append(np.average(np.array(last_r)))\n",
    "        hos.append(np.mean(last_r_hos))\n",
    "        last_r = []\n",
    "        last_r_hos = []\n",
    "        \n",
    "    if episode>50:\n",
    "        agent.training_step(num_training_episode)\n",
    "        \n",
    "        if rewards[-1]>max_reward and episode>1000:\n",
    "            max_reward = rewards[-1]\n",
    "            agent.model.save_weights(path)\n",
    "            print(f'Saved new weights for reward of {max_reward}')\n",
    "            \n",
    "    \n",
    "    if episode%target_model_update==0:\n",
    "        agent.target_model.set_weights(agent.model.get_weights())\n",
    "    \n",
    "    if episode%200==0:\n",
    "        agent.epsilon*=0.9\n",
    "        agent.epsilon = min(agent.epsilon, 0.05)\n",
    "            \n",
    "    if episode%1000==0:\n",
    "        plt.plot(rewards)\n",
    "        plt.title('Average Rewards')\n",
    "        plt.show()\n",
    "        plt.plot(hos)\n",
    "        plt.title('Average Handovers')\n",
    "        plt.show()\n",
    "        \n",
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "print(rewards[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.load_weights(path)\n",
    "hos = []\n",
    "rewards = []\n",
    "agent.epsilon = 0\n",
    "for episode in tqdm(range(2000)):\n",
    "    src,dest = env.give_src_dest()\n",
    "    route = env.compute_route(src, dest)\n",
    "    state = route.popleft()\n",
    "    depth = 3*NUM_BASE_STATIONS\n",
    "    one_hot_cell = make_one_hot(env.sector_cells[src][0][0], depth)\n",
    "    one_hot_direction = make_one_hot(state[-1]+1, 8)\n",
    "    state = state[:-1]\n",
    "    state.extend(one_hot_direction)\n",
    "    state.extend(one_hot_cell) #Setting strongest cell as the initial serving cell (one_hot)\n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    num_hos = 0\n",
    "    \n",
    "    while done==0:\n",
    "        next_state, reward, done, change = agent.play_one_step(state, route, dest, Wrsrp, Who)\n",
    "        total_reward+=reward\n",
    "        state = next_state\n",
    "        if change:\n",
    "            num_hos +=1\n",
    "    rewards.append(total_reward)\n",
    "    hos.append(num_hos)\n",
    "\n",
    "print(np.mean(rewards))\n",
    "print(np.mean(hos))\n",
    "\n",
    "agent.mean_reward = np.mean(rewards) \n",
    "agent.mean_hos = np.mean(hos)\n",
    "\n",
    "print(agent.mean_reward)\n",
    "print(agent.mean_hos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent_baseline = Agent(6, NUM_BASE_STATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agent_baseline\n",
    "Wrsrp = 1\n",
    "Who = 0\n",
    "hos = []\n",
    "rewards = []\n",
    "agent.epsilon = 0\n",
    "for episode in tqdm(range(2000)):\n",
    "    src,dest = env.give_src_dest()\n",
    "    route = env.compute_route(src, dest)\n",
    "    state = route.popleft()\n",
    "    depth = 3*NUM_BASE_STATIONS\n",
    "    one_hot_cell = make_one_hot(env.sector_cells[src][0][0], depth)\n",
    "    one_hot_direction = make_one_hot(state[-1]+1, 8)\n",
    "    state = state[:-1]\n",
    "    state.extend(one_hot_direction)\n",
    "    state.extend(one_hot_cell)#Setting strongest cell as the initial serving cell (one_hot)\n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    num_hos = 0\n",
    "    \n",
    "    while done==0:\n",
    "        next_state, reward, done, change = agent.play_one_step(state, route, dest, Wrsrp, Who, baseline = True)\n",
    "        total_reward+=reward\n",
    "        state = next_state\n",
    "        if change:\n",
    "            num_hos +=1\n",
    "    rewards.append(total_reward)\n",
    "    hos.append(num_hos)\n",
    "\n",
    "print(np.mean(rewards))\n",
    "print(np.mean(hos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
